ID3：
if 训练集中M个样本已属同一类别：
    T.类别=这个类别
    return T
elif 训练集中M个样本不属于同一类，但是特征集合只含单个特征:
    T.类别=实例数最大的类
    return T
Fn=max(各个特征的信息增益)
if Fn<ε:
    T.类别=实例数最大的类
    return T
else:
    分成不同的子类别Di，每个子类别产生一棵树的子节点。
C4.5：
    1.优先选取取值种类较多的特征：使用信息增益比
    2.不能处理连续值特征：将连续的特征离散化
    3.决策树容易过拟合：引入正则化项
CART：二叉树，可用于回归
    1.特征选择过程对数计算过于复杂：采用基尼系数作为特征选取标准（分类）采用平方误差作为标准（回归）。
    2.对特征划分过于迅速：采用二叉树来对每一个特征进行划分。
        当一特征是离散值时{1,2,3}，CART按照取组合的形式划分集合为12和3,13和2，23和1,分别计算对应基尼系数或平方误差，选取误差最小的划分组合进行二切分。
        当一特征是连续值时，先将特征按从小到大顺序排列好，然后依次取每两个相邻值的作为划分点，然后比较这些划分点对应的基尼系数或平方误差。选取误差最小的划分来生成二叉子树。
    3.不能处理回归问题：特征选择直接使用均方误差就行，即计算每一次特征划分后的结果与实际结果值之间的均方误差。结果处理每个叶子节点对应的结果值就取该叶子节点中所有标本点标签值的均值（中位数）
